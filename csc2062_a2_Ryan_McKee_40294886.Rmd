---
title: "Assignment 2: Feature Engineering, Statistical Analysis and Machine Learning"
author: "Ryan McKee (40294886)"
output:
  pdf_document:
     includes: 
         in_header: "preamble.tex"
  html_notebook: default
  html_document:
    df_print: paged
---

## Introduction

The overall objective of this project is to create A.I that will be able to recognize letters in the set {a,b,c,d,e,f,g,h,i,j}, as well as recognize smiley faces, sad faces and exclamation
marks. In this part of the assignment(assignment 2) My goals are to create a data set of hand drawn letters, smiley faces, sad faces and exclamation marks. Then perform feature engineering on
these images to calculate features such as how many black pixels exist in the image and aspect ratio. Then perform statistical analysis of the data sets using methods of statistical inference such
as using histograms and finally for this assignment implement and evaluate some machine learning models that perform classification on the data set.

# Section 1

I created my data set images for letters in set {a,...,j}, smiley faces, sad faces and exclamation marks using GIMP with images of 18x18 pixels and exporting these images as PGM files type ASCII. This 
file type stored the values of pixels on 324 separate lines each line having a value of 255 representing a white pixel or 0 representing a black pixel. In order to convert these PGM files to a matrix
representation of the image in a csv file so that these images could be more easily analysed later in the project I created an R script as shown below. This R script stores each of the files in the PGM 
image folder as a list. I then looped through this list reading the pixel values of each of the element and changing the elements value in the matrix to 0 if the element value is greater or equal to 128
or if less than I changed the value of that element in the matrix to be equal to 1. I then used write.table to write the updated matrix's to new csv file's in a folder that contains the updated data set,
and using the gsub function in the paste function for getting the file path I changed the file extensions for the file names to go from .pgm to .csv.

```{r, eval=FALSE}
  {library(readr)
  library(utile.tables)
  
  
  # * To change the folder that contains the .pgm files
  # Just change the path in list.files() belowe to be equal
  # to the file path of the folder, and change the working_folder
  # to be equal to the file path + / *
  data_files <- list.files(path = "./dataset2/pmg_images_dataset")
  working_folder <- "./dataset2/pmg_images_dataset/"
  
  for(index in 1:length(data_files)){
    
    current_file <- data_files[index]
    
    current_file_path <- paste(working_folder, current_file, sep = "")
    
    pgmFile <- read_lines(file = current_file_path, skip = 4)
    imageMatrix <- matrix(pgmFile, nrow = 18, ncol = 18)
    
    for(row in 1:nrow(imageMatrix)) 
    {
      for(col in 1:ncol(imageMatrix)) 
      {
        if(imageMatrix[row,col] < 128)
        {
          imageMatrix[row,col] = 1
        }
        else
        {
          imageMatrix[row,col] = 0
        }
      }
    }
    
    imageMatrix <- t(imageMatrix)
    
    # * To change the folder that the updated dataset gets created in just change the first argument in the paste function to the file path for updated folder + / *
    write.table(imageMatrix, file = paste("./dataset2/csv_images_dataset", gsub('.pgm', '.csv', data_files[index]), sep = ""), col.names = F, row.names = F, sep = ",", quote = FALSE)
    
  }
  
  print("csv dataset has been successfully created.")
}
```


# Section 2
The objective of section 2 was to calculate an array of features describing each of the image matrix's created in part one. To do this I created 18 functions for each of the calculations
each function took a parameter relating to the calculation that the function was carrying out, for example all 16 features that were being calculated took an image matrix as a parameter
while the first two function that where getting the label and the index of the image matrix took the image matrix file's name so that I could take a sub string from from the file name.
I used these 18 functions in a for loop that iterated through a list of all the csv image files. Every iteration through this for loop creates an 18x18 matrix which reads the current file data into the matrix.
I then created an 18x1 matrix which stores the values returned from the calculation functions inside it. Finally I append the feature calculations matrix to a csv file so each row stores a the calculations for a new image file.

The custom function was the 18th function created for the feature calculations for this function I has a few ideas for how to implement this but decided that this function should check the center 6x6 subsset of the image matrix calculate the percentage of black pixels from the image that exist in this area. I think this feature may be useful as it could provide clear distinction between items in the dataset that are likely to have very little pixels in the center 6x6 like an a would probably no pixels in the center 6x6 if it is drawn big enough while a letter like an i would probably have a high percentage of black pixels in this area.

When working on section 3 I noticed that my histograms where off for eyes as I was getting 3 non letters that had more than 1 eye as sad faces and smiley faces should not have any especially the ones that where in my data set as I checked I realised that raster and clump was recognising an enclosed area in the image as an eye because the mouth of the image was reaching the bottom border of the page, to fix this I added 2 to each row and column in my raster image after which I got new updated results which were correct.

```{r, eval = FALSE}
source('./section2_code.R')
```



# Section 3

The objective of this section is to perform statistical analyses of the feature data created in from my data-set in section 2 to allow for important distinctions between handwritten symbols from my data-set.

## Section 3.1

In this subsection I have created 6 histograms histograms for the first six features over the full set of the 140 items in the data-set. I done this by reading my feature calculations csv file and adding it to a data.frame after which I created a function which created the histograms using the first 6 feature columns using ggplots.

Your work for this subsection here.
```{r, eval = TRUE, stdout = TRUE}
{
  library(readr)
  library(ggplot2)
  library(tidyverse)
  # matrix stats has functions for getting colMedian and standard deviation
  library(matrixStats)
  library(knitr) # using this to make a table for my report.
  
  # This section of code read in the feature calculations created in sections 2 code adds
  # csv seperated values to columns, and new lines create a new row in a data frame
  mydata <- read.csv("40294886_features.csv", row.names = NULL)
  calculated_features <- data.frame(mydata)
  calculated_features_col_names <- c("Label", "Index", "nr_pix", "rows_with_1", "cols_with_1",
    "rows_with_3p", "cols_with_3p", "aspect_ratio", "neigh_1",
    "no_neigh_above","no_neigh_below","no_neigh_left","no_neigh_right",
    "no_neigh_horiz", "no_neigh_vert","connected_areas","eyes","custom")
  colnames(calculated_features) <- calculated_features_col_names

  # had to change custom function to numeric class as for some reason it was character
  mydata[ , 3:18] <- as.numeric(mydata[ , 3,18])
  
  
  
  # 3.1

 # plots histograms for the values for each of the features for each of the images
  histograms_3_1 <- function(){
    nr_pix_hist <- ggplot(data = calculated_features, aes(nr_pix))+
    geom_histogram(binwidth = 2, fill = "grey")+
    theme_bw() +
    labs(title = "Number of pixels histogram",
         x = "Number of pixels",
         y = "Frequency")
    
    rows_with_1_hist <- ggplot(data = calculated_features, aes(rows_with_1)) +
             geom_histogram(binwidth = 1, fill = "red") +
             theme_bw() +
             labs(title = "Rows with 1 pixel histogram",
                  x = "Number of rows with 1 pixel",
                  y = "Frequency")

    cols_with_1_hist <- calculated_features %>%
      ggplot(aes(cols_with_1))+
      geom_histogram(binwidth = 1, fill = "red")+
      theme_bw() +
      labs(title = "Columns with 1 histogram",
           x = "Number of columns with 1",
           y = "Frequency")

    rows_with_3p_hist <- calculated_features %>%
      ggplot(aes(rows_with_3p))+
      geom_histogram(binwidth = 1, fill = "blue")+
      theme_bw() +
      labs(title = "Rows with 3 pixels of more histogram",
           x = "Number of rows with 3 pixels or more",
           y = "Frequency")

    cols_with_3p_hist <- calculated_features %>%
      ggplot(aes(cols_with_3p))+
      geom_histogram(binwidth = 1, fill = "blue")+
      theme_bw() +
      labs(title = "Columns with 3 pixels of more histogram",
           x = "Number of columns with 3 pixels or more",
           y = "Frequency")

    aspect_ratio_hist <- calculated_features %>%
      ggplot(aes(aspect_ratio))+
      geom_histogram(binwidth = 0.1, fill = "orange")+
      theme_bw() +
      labs(title = "Aspect ratio histogram",
           x = "Aspect ratios",
           y = "Frequency")
    
    print(nr_pix_hist)
    print(rows_with_1_hist)
    print(cols_with_1_hist)
    print(rows_with_3p_hist)
    print(cols_with_3p_hist)
    print(aspect_ratio_hist)
  }
  
  histograms_3_1()

}
```
Number of pixels histogram
nr_pix historam seems to have a slightly random distribution although it has 4 clear peaks. The 0-20  pixels group contains the first peak. This group has around 20 images due to the pixel number being so low I think it is safe to assume that these are the i and ! The next group is 20-50 there is 2 peaks I assume that this group represents the letters with a vast majority of letters being around 30 pixels. After this group there is just random clusters of bars between 50-100 pixels I believe that these images as smiley faces and sad faces with maybe some thicker letters are in this group too. 

Rows with 1 pixel
rows_with_1 histogram seems to have a right skewed shape with some drops in frequency of some rows with 1 pixel numbers. The highest peak appears where number of rows wih 1 pixel is equal to 0 I believe that the frequency is high because there is multiple letters and potentailly non letters which are in this group some letters I think would be in the 0 rows_with_1 group would be a and c. After this there is a group from 1-5 rows with 1 I believe this group i comprised of b, e, d, e, h the final group is contains 5-13 pixels these are likey f, i, j, smiley face, sad face and exclamation marks.

Columns with 1 histogram
over 60 images in this histograms contain 0 columns with only 1, I believe his group comprises of a, b, c, d, e, g, h, i, !, as for the group groups

## Section 3.2

In this section the aim is to gain useful summary statistics like the mean, median and standard deviation of both the letters and non letters. To do this I continued writing the script I had created in section 3.1 using the calculated_features data.frame I had read in from the calculated features csv file I created in section 2 I split the data up into letters calculated features and a non letters calculated feature data frame. and carried out calculations on each of the features in these data frames to do this I use function I found from r documentation to get the column median, mean, and standard deviation of each column and then created new vectors from these containing the results of each of the columns for the three calculations I carried out. To make this data easier to read I then outputted it to a table using a knitr library function I found on youtube. (https://www.youtube.com/watch?v=hNgeVLotABg)

```{r, eval=TRUE, results='axis', stdout=TRUE}
{
  library(readr)
  library(ggplot2)
  # matrix stats has functions for getting colMedian and standard deviation
  library(matrixStats)
  library(knitr) # using this to make a table for my report.
  
  mydata <- read.csv("40294886_features.csv", row.names = NULL)
  calculated_features <- data.frame(mydata)
  colnames(calculated_features) <- c("Label", "Index", "nr_pix", "rows_with_1", "cols_with_1",
                                     "rows_with_3p", "cols_with_3p", "aspect_ratio", "neigh_1",
                                     "no_neigh_above","no_neigh_below","no_neigh_left","no_neigh_right",
                                     "no_neigh_horiz", "no_neigh_vert","connected_areas","eyes","custom")
  
  mydata[ , 3:18] <- as.numeric(mydata[ , 3,18])
  
  # 3.2
  letters_calculated_features <- calculated_features[1:80,]
  non_letters_calculated_features<- calculated_features[81:140,]
  letters_calculated_features[, 18] <- as.numeric(letters_calculated_features[ , 18])
  non_letters_calculated_features[ , 18] <- as.numeric(non_letters_calculated_features[ , 18])
  
  # # get the mean of each column
  letters_mean <- c(colMeans(letters_calculated_features[ , 3:18]))
  non_letters_mean <- c(colMeans(non_letters_calculated_features[ , 3:18]))
  
  # get the medians of each column
  letters_median <- c(colMedians(as.matrix(letters_calculated_features[ , 3:18])))
  non_letters_median <- c(colMedians(as.matrix(non_letters_calculated_features[ , 3:18])))

  #get the standard deviation of each column
  letters_sd <- colSds(as.matrix(letters_calculated_features[, 3:18][sapply(letters_calculated_features[ , 3:18 ], is.numeric)]))
  non_letters_sd <- colSds(as.matrix(non_letters_calculated_features[ , 3:18][sapply(non_letters_calculated_features[ , 3:18], is.numeric)]))
  
  # printing tables containing the results for standard_deviation, mean, median for both letters and
  # non letters.
  # source for making tables: https://www.youtube.com/watch?v=hNgeVLotABg
   table = matrix(NA, nrow = 16, ncol = 0)
  rownames(table) = c("nr_pix", "rows_with_1", "cols_with_1",
                                     "rows_with_3p", "cols_with_3p", "aspect_ratio", "neigh_1",
                                     "no_neigh_above","no_neigh_below","no_neigh_left","no_neigh_right",
                                     "no_neigh_horiz", "no_neigh_vert","connected_areas","eyes","custom")
  
  table <- cbind(table, letters_mean)
  table <- cbind(table, non_letters_mean)
  table <- cbind(table, letters_median)
  table <- cbind(table, non_letters_median)
  table <- cbind(table, letters_sd)
  table <- cbind(table, non_letters_sd)
  
  features_table<-kable(table, caption = "This table shows the letter and non letters mean, median and standard deviation for each of the feature calculations")
  
  print(features_table)
  
   # box plot for comparing all the summary data, to show in report
  summary_boxplot <- boxplot(letters_mean, non_letters_mean, letters_median, non_letters_median, letters_sd, non_letters_sd,
          main = "summary data comparisons",
          at = c(1,2,3,4,5,6),
          names = c("letter mean", "non letter mean", "letters median", "non letters median", "letters standard deviation", "non letters standard deviation"),
          las = 2,
          col = c("orange","red"),
          border = "brown",
          horizontal = TRUE,
          notch = FALSE
  )
}

```
In the above summary statistics presented in the table above there appears to already be clear differences in the mean standard deviation and medians between the majority of the letter and non letter features. Based on the values presented however it is clear the most substantial differences in values which could be used to differentiate between letters and non letters occur in the cols_with_1 feature where the mean of letters is over 2 lower and the median is 4 lower then non letters and with a 1.6 standard deviation for letters and 2.9 standard deviation for non_letter although by itself it could not accurately be used to decide by itself whether an image is a letter or non letter. this feature will be a strong used along side other features. The second strong feature which could be used would be no_neigh_vert there is a substantial difference between the mean and median which for letter is almost 3 higher than non letters and the statistics would be fairly accurate as the standard deviation is not too high with only a 1.4 difference between letters and non letters. The last feature which appers useful is eyes for each of the non letters values for mean, median and standard deviation it is equal to 0 meaning that no non letters have eyes, while the ean for letters eyes is 0.5 with a standard deviation of 0.5 so any image that has eyes can be pretty much guarenteed to be a letter.

## Section 3.3

Your work for this subsection here.

## Section 3.4

Your work for this subsection here.

# Section 4

## Section 4.1

Your work for this subsection here.

## Section 4.2

Your work for this subsection here.

## Section 4.3

Your work for this subsection here.

## Section 4.4

Your work for this subsection here.