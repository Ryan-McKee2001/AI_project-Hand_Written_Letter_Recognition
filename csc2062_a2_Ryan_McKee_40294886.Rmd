---
title: "Assignment 2: Feature Engineering, Statistical Analysis and Machine Learning"
author: "Ryan McKee (40294886)"
output:
  pdf_document:
     includes: 
         in_header: "preamble.tex"
  html_notebook: default
  html_document:
    df_print: paged
---

## Introduction

The overall objective of this project is to create A.I that will be able to recognize letters in the set {a,b,c,d,e,f,g,h,i,j}, as well as recognize smiley faces, sad faces and exclamation
marks. In this part of the assignment(assignment 2) My goals are to create a data set of hand drawn letters, smiley faces, sad faces and exclamation marks. Then perform feature engineering on
these images to calculate features such as how many black pixels exist in the image and aspect ratio. Then perform statistical analysis of the data sets using methods of statistical inference such
as using histograms and finally for this assignment implement and evaluate some machine learning models that perform classification on the data set.

# Section 1

I created my data set images for letters in set {a,...,j}, smiley faces, sad faces and exclamation marks using GIMP with images of 18x18 pixels and exporting these images as PGM files type ASCII. This 
file type stored the values of pixels on 324 separate lines each line having a value of 255 representing a white pixel or 0 representing a black pixel. In order to convert these PGM files to a matrix
representation of the image in a csv file so that these images could be more easily analysed later in the project I created an R script as shown below. This R script stores each of the files in the PGM 
image folder as a list. I then looped through this list reading the pixel values of each of the element and changing the elements value in the matrix to 0 if the element value is greater or equal to 128
or if less than I changed the value of that element in the matrix to be equal to 1. I then used write.table to write the updated matrix's to new csv file's in a folder that contains the updated data set,
and using the gsub function in the paste function for getting the file path I changed the file extensions for the file names to go from .pgm to .csv.

```{r}
  {library(readr)
  library(utile.tables)
  
  
  # * To change the folder that contains the .pgm files
  # Just change the path in list.files() belowe to be equal
  # to the file path of the folder, and change the working_folder
  # to be equal to the file path + / *
  data_files <- list.files(path = "./dataset2/pmg_images_dataset")
  working_folder <- "./dataset2/pmg_images_dataset/"
  
  for(index in 1:length(data_files)){
    
    current_file <- data_files[index]
    
    current_file_path <- paste(working_folder, current_file, sep = "")
    
    pgmFile <- read_lines(file = current_file_path, skip = 4)
    imageMatrix <- matrix(pgmFile, nrow = 18, ncol = 18)
    
    for(row in 1:nrow(imageMatrix)) 
    {
      for(col in 1:ncol(imageMatrix)) 
      {
        if(imageMatrix[row,col] < 128)
        {
          imageMatrix[row,col] = 1
        }
        else
        {
          imageMatrix[row,col] = 0
        }
      }
    }
    
    imageMatrix <- t(imageMatrix)
    
    # * To change the folder that the updated dataset gets created in just change the first argument in the paste function to the file path for updated folder + / *
    write.table(imageMatrix, file = paste("./dataset2/csv_images_dataset", gsub('.pgm', '.csv', data_files[index]), sep = ""), col.names = F, row.names = F, sep = ",", quote = FALSE)
    
  }
  
  print("csv dataset has been successfully created.")
}
```


# Section 2
The objective of section 2 was to calculate an array of features describing each of the image matrix's created in part one. To do this I created 18 functions for each of the calculations
each function took a parameter relating to the calculation that the function was carrying out, for example all 16 features that were being calculated took an image matrix as a parameter
while the first two function that where getting the label and the index of the image matrix took the image matrix file's name so that I could take a sub string from from the file name.
I used these 18 functions in a for loop that iterated through a list of all the csv image files. Every iteration through this for loop creates an 18x18 matrix which reads the current file data into the matrix.
I then created an 18x1 matrix which stores the values returned from the calculation functions inside it. Finally I append the feature calculations matrix to a csv file so each row stores a the calculations for a new image file.

The custom function was the 18th function created for the feature calculations for this function I has a few ideas for how to implement this but decided that this function should check the center 6x6 subsset of the image matrix calculate the percentage of black pixels from the image that exist in this area. I think this feature may be useful as it could provide clear distinction between items in the dataset that are likely to have very little pixels in the center 6x6 like an a would probably no pixels in the center 6x6 if it is drawn big enough while a letter like an i would probably have a high percentage of black pixels in this area.

```{r}
source('./section2_code.R')
```



# Section 3

## Section 3.1

Your work for this subsection here.
```{r, echo=TRUE}
source('./section3_code.R')
```


## Section 3.2

Your work for this subsection here.

## Section 3.3

Your work for this subsection here.

## Section 3.4

Your work for this subsection here.

# Section 4

## Section 4.1

Your work for this subsection here.

## Section 4.2

Your work for this subsection here.

## Section 4.3

Your work for this subsection here.

## Section 4.4

Your work for this subsection here.